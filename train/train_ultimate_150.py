# Path setup - MUST be first before any imports
import os
import sys
_script_dir = os.path.dirname(os.path.abspath(__file__))
_project_root = os.path.dirname(_script_dir)
if _project_root not in sys.path:
    sys.path.insert(0, _project_root)

"""
Train DreamerV3 Agent with ULTIMATE 150+ Features

This is the MAXIMUM PERFORMANCE training script that uses:
- 152 total features from ALL sources
- Multi-timeframe: M5, M15, H1, H4, D1, W1
- Cross-timeframe intelligence
- Enhanced macro correlations (24 features)
- Advanced economic calendar (8 features)
- Market microstructure (12 features)

Updated with PPO Gold Trading Improvements:
- Advanced 5-component reward function
- Differential Sharpe Ratio (DSR)
- Optimized gamma=0.6 for intraday-swing

Expected performance: 80-120%+ annual return, 3.5-4.5+ Sharpe ratio
"""

import argparse
import numpy as np
import torch
from tqdm import tqdm
import logging
import json
from datetime import datetime

from features.ultimate_150_features import make_ultimate_features
from models.dreamer_agent import DreamerV3Agent
from env.gold_trading_env import GoldTradingEnvDreamer

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ============================================================================
# Performance Metrics Calculator
# ============================================================================
def calculate_performance_metrics(equities, positions, returns_list):
    """
    Calculate comprehensive trading performance metrics.

    Args:
        equities: List of equity values
        positions: List of positions (0 or 1)
        returns_list: List of step returns

    Returns:
        Dict with all performance metrics
    """
    equities = np.array(equities)
    positions = np.array(positions)
    returns_arr = np.array(returns_list)

    # Basic returns
    total_return = (equities[-1] / equities[0] - 1) * 100 if len(equities) > 0 else 0

    # Annualized return (assuming M5 = 288 bars/day, 252 trading days)
    n_bars = len(equities)
    bars_per_year = 288 * 252  # M5 timeframe
    years = n_bars / bars_per_year
    annualized_return = ((equities[-1] / equities[0]) ** (1/years) - 1) * 100 if years > 0 else 0

    # Sharpe Ratio (annualized)
    if len(returns_arr) > 1 and np.std(returns_arr) > 0:
        sharpe = np.mean(returns_arr) / np.std(returns_arr) * np.sqrt(bars_per_year)
    else:
        sharpe = 0

    # Sortino Ratio (annualized)
    negative_returns = returns_arr[returns_arr < 0]
    if len(negative_returns) > 0:
        downside_std = np.sqrt(np.mean(negative_returns ** 2))
        sortino = np.mean(returns_arr) / downside_std * np.sqrt(bars_per_year) if downside_std > 0 else 0
    else:
        sortino = sharpe * 1.5  # If no negative returns, estimate

    # Maximum Drawdown
    peak = np.maximum.accumulate(equities)
    drawdown = (peak - equities) / peak
    max_drawdown = np.max(drawdown) * 100

    # Trade statistics
    position_changes = np.diff(positions)
    trades = int(np.sum(np.abs(position_changes) > 0))

    # Time in market
    time_in_market = np.mean(positions) * 100

    # Win rate (bars with positive return while in position)
    in_position_returns = returns_arr[positions[:-1] == 1] if len(positions) > 1 else np.array([])
    if len(in_position_returns) > 0:
        win_rate = np.mean(in_position_returns > 0) * 100
        avg_win = np.mean(in_position_returns[in_position_returns > 0]) * 100 if np.any(in_position_returns > 0) else 0
        avg_loss = np.mean(in_position_returns[in_position_returns < 0]) * 100 if np.any(in_position_returns < 0) else 0
    else:
        win_rate = 0
        avg_win = 0
        avg_loss = 0

    # Profit Factor
    gross_profit = np.sum(in_position_returns[in_position_returns > 0]) if len(in_position_returns) > 0 else 0
    gross_loss = abs(np.sum(in_position_returns[in_position_returns < 0])) if len(in_position_returns) > 0 else 0
    profit_factor = gross_profit / gross_loss if gross_loss > 0 else float('inf')

    # Calmar Ratio
    calmar = annualized_return / max_drawdown if max_drawdown > 0 else 0

    return {
        'total_return': total_return,
        'annualized_return': annualized_return,
        'sharpe_ratio': sharpe,
        'sortino_ratio': sortino,
        'max_drawdown': max_drawdown,
        'calmar_ratio': calmar,
        'profit_factor': profit_factor,
        'trades': trades,
        'time_in_market': time_in_market,
        'win_rate': win_rate,
        'avg_win': avg_win,
        'avg_loss': avg_loss,
        'final_equity': equities[-1] if len(equities) > 0 else 1.0,
    }

# Environment settings
WINDOW = 128  # 128 barras = 10.6 horas de contexto M5 (sesiÃ³n completa)
COST = 0.0001
TRAIN_END_DATE = "2022-01-01"

# DreamerV3 hyperparameters
BATCH_SIZE = 16
PREFILL_STEPS = 5_000  # Random exploration to fill buffer
TRAIN_STEPS = 1_000_000  # Training steps
TRAIN_EVERY = 4  # Train every N environment steps
# Save every 50,000 steps to avoid Google Drive quota limits
# For 1M training: 20 checkpoints
SAVE_EVERY = 50_000

# Use absolute path based on script location
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
SAVE_DIR = os.path.join(SCRIPT_DIR, "dreamer_ultimate")
SAVE_PREFIX = "ultimate_150_xauusd"


class TradingEnvironment:
    """
    Trading environment for DreamerV3 with Ultimate 150+ features
    and Advanced Reward Function.

    Features:
    - 5-component reward function (Sortino, DD penalty, TC penalty, Holding bonus, Breach penalty)
    - Differential Sharpe Ratio (DSR)
    - Drawdown tracking
    """
    def __init__(
        self,
        features,
        returns,
        window=64,
        cost_per_trade=0.0001,
        max_drawdown=0.30,
        use_advanced_reward=True,
    ):
        self.X = features.astype(np.float32)
        self.r = returns.astype(np.float32)
        self.window = int(window)
        self.cost = float(cost_per_trade)
        self.max_drawdown = float(max_drawdown)
        self.use_advanced_reward = use_advanced_reward
        self.T = len(self.r)

        # DSR calculator
        self.dsr = DifferentialSharpeRatio(eta=0.01)

        logger.info(f"Environment initialized:")
        logger.info(f"  â€¢ Features: {self.X.shape}")
        logger.info(f"  â€¢ Window: {self.window}")
        logger.info(f"  â€¢ Cost: {self.cost:.4f}")
        logger.info(f"  â€¢ Max Drawdown: {self.max_drawdown:.1%}")
        logger.info(f"  â€¢ Advanced Reward: {self.use_advanced_reward}")
        logger.info(f"  â€¢ Total steps: {self.T:,}")

        self.reset()

    def reset(self):
        """Reset environment"""
        self.t = self.window
        self.pos = 0  # 0 = flat, 1 = long
        self.equity = 1.0
        self.peak_equity = 1.0
        self.position_entry_step = None
        self.unrealized_pnl = 0.0
        self.returns_history = []

        # Reset DSR
        self.dsr.reset()

        return self._get_obs()

    def _get_obs(self):
        """
        Get current observation

        Returns:
            Flattened observation with:
            - Last WINDOW timesteps of features
            - Current position
        """
        # Get window of features
        w = self.X[self.t - self.window : self.t]  # (window, num_features)

        # Flatten
        obs = np.concatenate([w.reshape(-1), np.array([self.pos], dtype=np.float32)])

        return obs.astype(np.float32)

    def _get_current_drawdown(self) -> float:
        """Calculate current drawdown from peak"""
        self.peak_equity = max(self.peak_equity, self.equity)
        if self.peak_equity > 0:
            return (self.peak_equity - self.equity) / self.peak_equity
        return 0.0

    def _get_position_duration(self) -> int:
        """How many steps the position has been open"""
        if self.position_entry_step is None:
            return 0
        return self.t - self.position_entry_step

    def step(self, action_onehot):
        """
        Execute action with advanced reward function.

        Args:
            action_onehot: one-hot encoded action [flat, long]

        Returns:
            obs, reward, done, info
        """
        # Decode action (for long-only: 0=flat, 1=long)
        new_pos = int(np.argmax(action_onehot))  # 0 or 1

        # Ensure long-only
        new_pos = max(0, min(1, new_pos))

        # Position change
        delta = abs(new_pos - self.pos)

        # Costs
        trade_cost = self.cost * delta

        # PnL from holding previous position
        step_return = self.pos * self.r[self.t]

        # Track position entry/exit
        if delta > 0:
            if new_pos != 0:
                self.position_entry_step = self.t
                self.unrealized_pnl = 0.0
            else:
                self.position_entry_step = None
                self.unrealized_pnl = 0.0

        # Update unrealized PnL
        if self.pos != 0:
            self.unrealized_pnl += step_return

        # Store return history
        self.returns_history.append(step_return)
        if len(self.returns_history) > 100:
            self.returns_history = self.returns_history[-100:]

        # Calculate reward
        if self.use_advanced_reward:
            # DSR value
            dsr_value = self.dsr.calculate(step_return)

            # Current drawdown
            current_dd = self._get_current_drawdown()

            # Position duration
            position_duration = self._get_position_duration()

            # Is winner?
            is_winner = self.unrealized_pnl > 0

            # Advanced reward
            reward, _ = calculate_advanced_reward(
                portfolio_return=step_return,
                current_drawdown=current_dd,
                downside_returns=np.array(self.returns_history),
                position_duration=position_duration,
                is_winner=is_winner,
                transaction_cost=trade_cost,
                max_drawdown=self.max_drawdown,
                dsr_value=dsr_value,
            )
        else:
            # Simple reward
            reward = step_return - trade_cost

        # Update state
        self.equity *= (1 + step_return - trade_cost)
        self.pos = new_pos
        self.t += 1

        # Done
        done = (self.t >= self.T - 1)

        # Next observation
        obs = self._get_obs() if not done else np.zeros_like(self._get_obs())

        info = {
            'equity': self.equity,
            'position': self.pos,
            'step_return': step_return,
            'drawdown': self._get_current_drawdown(),
        }

        return obs, reward, done, info

    @property
    def observation_space(self):
        """Observation space dimension"""
        # Window * num_features + 1 (position)
        return self.window * self.X.shape[1] + 1

    @property
    def action_space(self):
        """Action space dimension (2 for long-only: flat or long)"""
        return 2


def main():
    parser = argparse.ArgumentParser(description='Train DreamerV3 with Ultimate 150+ Features')
    parser.add_argument('--steps', type=int, default=TRAIN_STEPS, help='Number of training steps')
    parser.add_argument('--batch-size', type=int, default=BATCH_SIZE, help='Batch size')
    parser.add_argument('--device', type=str, default='auto', help='Device: cuda/mps/cpu/auto')
    parser.add_argument('--resume', type=str, default=None, help='Resume from checkpoint')
    parser.add_argument('--base-tf', type=str, default='M5', help='Base timeframe (M5/M15/H1)')
    args = parser.parse_args()

    logger.info("="*70)
    logger.info("ðŸš€ ULTIMATE 150+ FEATURE TRAINING")
    logger.info("="*70)
    logger.info(f"Training steps: {args.steps:,}")
    logger.info(f"Batch size: {args.batch_size}")
    logger.info(f"Device: {args.device}")
    logger.info(f"Base timeframe: {args.base_tf}")
    logger.info("")

    # ========== LOAD ULTIMATE FEATURES ==========
    logger.info("ðŸ“Š Loading Ultimate 150+ features...")
    logger.info("-" * 70)

    X, returns, timestamps = make_ultimate_features(base_timeframe=args.base_tf)

    logger.info(f"\nâœ… Features loaded:")
    logger.info(f"  â€¢ Feature matrix: {X.shape}")
    logger.info(f"  â€¢ Returns: {returns.shape}")
    logger.info(f"  â€¢ Date range: {timestamps[0]} to {timestamps[-1]}")

    # ========== SPLIT TRAIN/VAL/TEST ==========
    logger.info("\nðŸ“… Splitting train/validation/test...")

    # Find split indices
    # Train: before 2022-01-01
    # Validation: 2022-01-01 to 2023-01-01
    # Test: after 2023-01-01
    VAL_END_DATE = "2023-01-01"

    train_mask = timestamps < TRAIN_END_DATE
    val_mask = (timestamps >= TRAIN_END_DATE) & (timestamps < VAL_END_DATE)
    test_mask = timestamps >= VAL_END_DATE

    train_idx = np.where(train_mask)[0][-1] if train_mask.any() else len(X) // 3
    val_idx = np.where(val_mask)[0][-1] if val_mask.any() else len(X) * 2 // 3

    X_train = X[:train_idx]
    r_train = returns[:train_idx]

    X_val = X[train_idx:val_idx]
    r_val = returns[train_idx:val_idx]

    X_test = X[val_idx:]
    r_test = returns[val_idx:]

    logger.info(f"  â€¢ Train samples: {len(X_train):,} ({timestamps[0]} to {timestamps[train_idx-1]})")
    logger.info(f"  â€¢ Val samples: {len(X_val):,} ({timestamps[train_idx]} to {timestamps[val_idx-1]})")
    logger.info(f"  â€¢ Test samples: {len(X_test):,} ({timestamps[val_idx]} to {timestamps[-1]})")

    # ========== LOAD PRICE DATA ==========
    logger.info("\nðŸ’° Loading price data for SL/TP...")

    import pandas as pd
    base_data_file = {
        'M5': 'xauusd_m5.csv',
        'M15': 'xauusd_m15.csv',
        'H1': 'xauusd_h1_from_m1.csv',
    }.get(args.base_tf, 'xauusd_m5.csv')

    df_prices = pd.read_csv(f"data/{base_data_file}")
    prices = df_prices['close'].values.astype(np.float32)

    # Split prices same as features
    p_train = prices[:train_idx]
    p_val = prices[train_idx:val_idx]
    p_test = prices[val_idx:]

    logger.info(f"  â€¢ Prices loaded: {len(prices):,} bars")

    # ========== CREATE ENVIRONMENT ==========
    logger.info("\nðŸŽ® Creating Professional Trading Environment...")
    logger.info(f"  Risk Management:")
    logger.info(f"    â€¢ Risk per trade: 2%")
    logger.info(f"    â€¢ Min profit target: 0.3%")
    logger.info(f"    â€¢ Stop Loss: 0.3% (1:1 RR)")
    logger.info(f"    â€¢ Take Profit: 0.9% (1:3 RR)")
    logger.info(f"    â€¢ Trailing activation: 0.3%")
    logger.info(f"    â€¢ Max Drawdown: 30% (HARD STOP)")

    env = GoldTradingEnvDreamer(
        features=X_train,
        returns=r_train,
        prices=p_train,
        window=WINDOW,
        initial_balance=10000.0,
        max_drawdown=0.30,
    )

    logger.info(f"\nâœ… Environment ready:")
    logger.info(f"  â€¢ Observation dim: {env.observation_space}")
    logger.info(f"  â€¢ Action dim: {env.action_space}")

    # ========== DEVICE SETUP ==========
    if args.device == 'auto':
        if torch.cuda.is_available():
            device = 'cuda'
        elif torch.backends.mps.is_available():
            device = 'mps'
        else:
            device = 'cpu'
    else:
        device = args.device

    logger.info(f"\nðŸ–¥ï¸  Using device: {device}")

    # ========== CREATE AGENT ==========
    logger.info("\nðŸ¤– Creating DreamerV3 agent...")

    agent = DreamerV3Agent(
        obs_dim=env.observation_space,
        action_dim=env.action_space,
        embed_dim=256,
        hidden_dim=512,
        stoch_dim=32,
        num_categories=32,
        device=device,
        gamma=0.6,        # âœ… Optimizado para intraday-swing (era 0.99)
        lambda_=0.95,
    )

    # Resume from checkpoint if specified
    if args.resume:
        logger.info(f"ðŸ“‚ Resuming from: {args.resume}")
        agent.load(args.resume)

    # ========== PREFILL REPLAY BUFFER ==========
    logger.info(f"\nðŸŽ² Prefilling replay buffer ({PREFILL_STEPS:,} steps)...")

    obs = env.reset()
    for _ in tqdm(range(PREFILL_STEPS), desc="Prefill"):
        # Random action
        action = np.random.randint(0, env.action_space)
        action_onehot = np.eye(env.action_space)[action]

        # Step
        next_obs, reward, done, info = env.step(action_onehot)

        # Store transition
        agent.replay_buffer.add(obs, action_onehot, reward, done)

        # Update
        obs = next_obs
        if done:
            obs = env.reset()

    logger.info(f"âœ… Replay buffer prefilled: {len(agent.replay_buffer)} transitions")

    # ========== TRAINING LOOP ==========
    logger.info(f"\nðŸ‹ï¸  Starting training for {args.steps:,} steps...")
    logger.info("-" * 70)

    os.makedirs(SAVE_DIR, exist_ok=True)

    obs = env.reset()
    h, z = None, None  # Initialize hidden state
    episode_reward = 0
    episode_count = 0
    best_reward = -np.inf

    # Training history for tracking
    training_history = {
        'steps': [],
        'episode_rewards': [],
        'losses': [],
        'equities': [],
    }
    recent_losses = []

    for step in tqdm(range(args.steps), desc="Training"):
        # Select action from agent (returns action and updated hidden state)
        action, (h, z) = agent.act(obs, h, z, deterministic=False)

        # Convert to discrete action index
        action_idx = np.argmax(action)
        action_onehot = np.eye(env.action_space)[action_idx]

        # Environment step
        next_obs, reward, done, info = env.step(action_onehot)

        # Store transition
        agent.replay_buffer.add(obs, action_onehot, reward, done)

        # Accumulate reward
        episode_reward += reward

        # Train agent every few steps
        if step % TRAIN_EVERY == 0:
            loss = agent.train_step(batch_size=args.batch_size)
            if loss:
                recent_losses.append(loss['world_model_loss'])

        # Episode end
        if done:
            episode_count += 1

            if episode_reward > best_reward:
                best_reward = episode_reward

            # Track episode
            training_history['steps'].append(step)
            training_history['episode_rewards'].append(episode_reward)
            training_history['equities'].append(info['equity'])
            if recent_losses:
                training_history['losses'].append(np.mean(recent_losses))
                recent_losses = []

            # Reset
            obs = env.reset()
            h, z = None, None  # Reset hidden state
            episode_reward = 0
        else:
            obs = next_obs

        # Save checkpoint with metrics
        if (step + 1) % SAVE_EVERY == 0:
            checkpoint_path = os.path.join(
                SAVE_DIR,
                f"{SAVE_PREFIX}_step{step+1}.pt"
            )
            agent.save(checkpoint_path)

            # Log progress
            avg_loss = np.mean(training_history['losses'][-10:]) if training_history['losses'] else 0
            avg_equity = np.mean(training_history['equities'][-10:]) if training_history['equities'] else 1

            logger.info(f"\nðŸ’¾ Checkpoint saved: {checkpoint_path}")
            logger.info(f"   Step: {step+1:,} | Episodes: {episode_count}")
            logger.info(f"   Best reward: {best_reward:.4f} | Avg equity: {avg_equity:.4f}")
            logger.info(f"   Avg loss: {avg_loss:.4f}")

    # ========== FINAL SAVE ==========
    final_path = os.path.join(SAVE_DIR, f"{SAVE_PREFIX}_final.pt")
    agent.save(final_path)

    # Save training history
    history_path = os.path.join(SAVE_DIR, f"{SAVE_PREFIX}_history.json")
    with open(history_path, 'w') as f:
        json.dump({
            'steps': training_history['steps'],
            'episode_rewards': training_history['episode_rewards'],
            'losses': training_history['losses'],
            'equities': training_history['equities'],
            'total_steps': args.steps,
            'total_episodes': episode_count,
            'best_reward': best_reward,
            'timestamp': datetime.now().isoformat(),
        }, f, indent=2)

    logger.info("\n" + "="*70)
    logger.info("âœ… TRAINING COMPLETE!")
    logger.info("="*70)
    logger.info(f"Final model saved: {final_path}")
    logger.info(f"Training history saved: {history_path}")
    logger.info(f"Best episode reward: {best_reward:.6f}")
    logger.info(f"Total episodes: {episode_count}")

    # ========== EVALUATION ON VALIDATION SET ==========
    logger.info("\n" + "="*70)
    logger.info("ðŸ“Š EVALUATING ON VALIDATION SET (2022)")
    logger.info("="*70)

    val_env = GoldTradingEnvDreamer(
        features=X_val, returns=r_val, prices=p_val,
        window=WINDOW, max_drawdown=0.30
    )
    val_equities, val_positions, val_returns = [], [], []

    obs = val_env.reset()
    h, z = None, None

    while True:
        action, (h, z) = agent.act(obs, h, z, deterministic=True)
        action_idx = np.argmax(action)
        action_onehot = np.eye(val_env.action_space)[action_idx]

        obs, reward, done, info = val_env.step(action_onehot)

        val_equities.append(info['equity'])
        val_positions.append(info['position'])
        val_returns.append(info['step_return'])

        if done:
            break

    val_metrics = calculate_performance_metrics(val_equities, val_positions, val_returns)

    logger.info(f"\nðŸ“ˆ Validation Results (2022):")
    logger.info(f"   Total Return: {val_metrics['total_return']:.2f}%")
    logger.info(f"   Annualized Return: {val_metrics['annualized_return']:.2f}%")
    logger.info(f"   Sharpe Ratio: {val_metrics['sharpe_ratio']:.2f}")
    logger.info(f"   Sortino Ratio: {val_metrics['sortino_ratio']:.2f}")
    logger.info(f"   Max Drawdown: {val_metrics['max_drawdown']:.2f}%")
    logger.info(f"   Calmar Ratio: {val_metrics['calmar_ratio']:.2f}")
    logger.info(f"   Profit Factor: {val_metrics['profit_factor']:.2f}")
    logger.info(f"   Trades: {val_metrics['trades']}")
    logger.info(f"   Win Rate: {val_metrics['win_rate']:.1f}%")
    logger.info(f"   Time in Market: {val_metrics['time_in_market']:.1f}%")

    # ========== EVALUATION ON TEST SET ==========
    logger.info("\n" + "="*70)
    logger.info("ðŸ“Š EVALUATING ON TEST SET (2023-2025)")
    logger.info("="*70)

    test_env = GoldTradingEnvDreamer(
        features=X_test, returns=r_test, prices=p_test,
        window=WINDOW, max_drawdown=0.30
    )
    test_equities, test_positions, test_returns = [], [], []

    obs = test_env.reset()
    h, z = None, None

    while True:
        action, (h, z) = agent.act(obs, h, z, deterministic=True)
        action_idx = np.argmax(action)
        action_onehot = np.eye(test_env.action_space)[action_idx]

        obs, reward, done, info = test_env.step(action_onehot)

        test_equities.append(info['equity'])
        test_positions.append(info['position'])
        test_returns.append(info['step_return'])

        if done:
            break

    test_metrics = calculate_performance_metrics(test_equities, test_positions, test_returns)

    logger.info(f"\nðŸ“ˆ Test Results (2023-2025 Out-of-Sample):")
    logger.info(f"   Total Return: {test_metrics['total_return']:.2f}%")
    logger.info(f"   Annualized Return: {test_metrics['annualized_return']:.2f}%")
    logger.info(f"   Sharpe Ratio: {test_metrics['sharpe_ratio']:.2f}")
    logger.info(f"   Sortino Ratio: {test_metrics['sortino_ratio']:.2f}")
    logger.info(f"   Max Drawdown: {test_metrics['max_drawdown']:.2f}%")
    logger.info(f"   Calmar Ratio: {test_metrics['calmar_ratio']:.2f}")
    logger.info(f"   Profit Factor: {test_metrics['profit_factor']:.2f}")
    logger.info(f"   Trades: {test_metrics['trades']}")
    logger.info(f"   Win Rate: {test_metrics['win_rate']:.1f}%")
    logger.info(f"   Time in Market: {test_metrics['time_in_market']:.1f}%")

    # Save all metrics
    metrics_path = os.path.join(SAVE_DIR, f"{SAVE_PREFIX}_metrics.json")
    with open(metrics_path, 'w') as f:
        json.dump({
            'validation': val_metrics,
            'test': test_metrics,
            'training': {
                'total_steps': args.steps,
                'total_episodes': episode_count,
                'best_episode_reward': best_reward,
            },
            'timestamp': datetime.now().isoformat(),
        }, f, indent=2)

    # Save equity curves for visualization
    curves_path = os.path.join(SAVE_DIR, f"{SAVE_PREFIX}_equity_curves.json")
    with open(curves_path, 'w') as f:
        json.dump({
            'validation_equity': val_equities,
            'test_equity': test_equities,
        }, f)

    logger.info(f"\nðŸ’¾ Metrics saved: {metrics_path}")
    logger.info(f"ðŸ’¾ Equity curves saved: {curves_path}")

    # ========== FINAL SUMMARY ==========
    logger.info("\n" + "="*70)
    logger.info("ðŸŽ¯ FINAL SUMMARY")
    logger.info("="*70)

    logger.info(f"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ULTIMATE 150+ MODEL RESULTS                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  VALIDATION (2022)           â”‚  TEST (2023-2025)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Return: {val_metrics['total_return']:>7.2f}%             â”‚  Return: {test_metrics['total_return']:>7.2f}%                    â”‚
â”‚  Sharpe: {val_metrics['sharpe_ratio']:>7.2f}              â”‚  Sharpe: {test_metrics['sharpe_ratio']:>7.2f}                     â”‚
â”‚  Max DD: {val_metrics['max_drawdown']:>7.2f}%             â”‚  Max DD: {test_metrics['max_drawdown']:>7.2f}%                    â”‚
â”‚  Trades: {val_metrics['trades']:>7}              â”‚  Trades: {test_metrics['trades']:>7}                     â”‚
â”‚  Win Rate: {val_metrics['win_rate']:>5.1f}%            â”‚  Win Rate: {test_metrics['win_rate']:>5.1f}%                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)

    logger.info("\nðŸŽ‰ Ultimate 150+ feature model is ready for deployment!")


if __name__ == '__main__':
    main()
